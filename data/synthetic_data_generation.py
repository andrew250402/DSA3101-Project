# -*- coding: utf-8 -*-
"""Synthetic Data Generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ascGfJEJeUaJ6xB9El766snMLPYXGyst
"""

# Install the Faker library (if not already installed)
!pip install faker

import numpy as np
import pandas as pd
from faker import Faker
import datetime

# ------------------------- #
# 1. SETUP & CONFIGURATION  #
# ------------------------- #

# Set random seeds for reproducibility
np.random.seed(42)
fake = Faker()
fake.seed_instance(42)

# Define the total number of synthetic customers
n_customers = 10000

# Define job categories and their associated probabilities.
jobs = ["admin", "technician", "entrepreneur", "blue-collar", "retired",
        "services", "self-employed", "management", "unemployed", "student"]
job_probs = [0.15, 0.15, 0.05, 0.20, 0.05, 0.15, 0.05, 0.10, 0.05, 0.05]
# Validate that job probabilities sum to 1.
if not np.isclose(sum(job_probs), 1):
    raise ValueError("Job probabilities do not sum to 1.")

# Pre-generate customer job choices based on defined probabilities.
job_choices = np.random.choice(jobs, size=n_customers, p=job_probs)

# Define region categories and their probabilities.
regions = ["Urban", "Suburban", "Rural"]
region_probs = [0.6, 0.3, 0.1]
if not np.isclose(sum(region_probs), 1):
    raise ValueError("Region probabilities do not sum to 1.")

# Define income ranges (monthly in USD) for each job category.
income_ranges = {
    "student": (0, 1500),
    "retired": (1500, 3000),
    "unemployed": (0, 1000),
    "admin": (3000, 5000),
    "technician": (2500, 4500),
    "entrepreneur": (1500, 10000),
    "blue-collar": (1000, 4000),
    "services": (1250, 3500),
    "self-employed": (2500, 6000),
    "management": (5000, 10000)
}

# Define education levels and conditional probabilities by job type.
education_levels = ["primary", "secondary", "tertiary"]
education_probs_by_job = {
    "admin":         [0.15, 0.50, 0.35],
    "technician":    [0.05, 0.35, 0.60],
    "entrepreneur":  [0.10, 0.30, 0.60],
    "blue-collar":   [0.40, 0.50, 0.10],
    "retired":       [0.30, 0.50, 0.20],
    "services":      [0.30, 0.50, 0.20],
    "self-employed": [0.10, 0.40, 0.50],
    "management":    [0.10, 0.30, 0.60],
    "unemployed":    [0.35, 0.45, 0.20],
    "student":       [0.05, 0.25, 0.70]
}
# Ensure every job in our list has income and education settings.
for job in jobs:
    if job not in income_ranges or job not in education_probs_by_job:
        raise ValueError(f"Job {job} is missing in income_ranges or education_probs_by_job.")

# Define anomaly injection rates:
# General anomaly rate for fields like age and income.
anomaly_rate_general = 0.01
# Anomaly rate for product ownership columns (force to "yes").
anomaly_rate_products = 0.03

# ------------------------------- #
# 2. HELPER FUNCTIONS & SEGMENT   #
# ------------------------------- #

def generate_skewed_date():
    """
    Generates a non-uniform account creation date within the past 5 years,
    skewed towards more recent dates.
    Occasionally injects a future date as an anomaly.
    """
    r = np.random.rand() ** 2  # Skew toward recent dates
    start_date = datetime.datetime.now() - datetime.timedelta(days=5*365)
    end_date = datetime.datetime.now()
    delta = end_date - start_date
    date = start_date + r * delta
    # Inject anomaly: With a small probability, set a future creation date.
    if np.random.rand() < anomaly_rate_general:
        date = datetime.datetime.now() + datetime.timedelta(days=np.random.randint(1, 10))
    return date

def assign_segment(age, job, education, region):
    """
    Assigns a customer segment based on age, job, education, and region.
    This rule-based segmentation can be further enhanced with clustering methods.
    """
    # High-Value segment for urban management with tertiary education.
    if job == "management" and education == "tertiary" and region == "Urban" and np.random.rand() < 0.7:
        return "High-Value"
    if age < 30:
        if region == "Urban":
            return np.random.choice(["Young Professionals", "Budget-Conscious"], p=[0.8, 0.2])
        else:
            return np.random.choice(["Young Professionals", "Budget-Conscious"], p=[0.6, 0.4])
    elif age <= 60:
        if region == "Urban":
            return np.random.choice(["Middle-Market", "Budget-Conscious", "High-Value"], p=[0.4, 0.3, 0.3])
        else:
            return np.random.choice(["Middle-Market", "Budget-Conscious", "High-Value"], p=[0.6, 0.3, 0.1])
    else:
        return np.random.choice(["Retired", "Budget-Conscious"], p=[0.7, 0.3])

# -------------------------- #
# 3. INITIALIZE DATA STORAGE #
# -------------------------- #

# Pre-generate sequential customer IDs.
customer_ids = np.arange(1, n_customers + 1)

# Initialize lists for core customer fields.
ages = []
maritals = []
educations = []
credit_defaults = []
segments = []
created_ats = []
incomes = []
customer_regions = []

# Initialize lists for banking product ownership.
credit_cards = []
personal_loans = []
mortgages = []
savings_accounts = []
investment_products = []
auto_loans = []
wealth_managements = []

# ------------------------ #
# 4. GENERATE CUSTOMER DATA #
# ------------------------ #

# Loop through each predetermined job to generate a customer profile.
for job in job_choices:
    # -- Region Assignment --
    # Randomly assign a region.
    region = np.random.choice(regions, p=region_probs)
    customer_regions.append(region)

    # -- Account Creation Date --
    # Generate a skewed creation date (with occasional anomaly).
    created_at = generate_skewed_date()
    created_ats.append(created_at)

    # -- Age Generation --
    # Generate age based on job type.
    if job == "student":
        age = np.random.randint(18, 26)
    elif job == "retired":
        age = np.random.randint(60, 90)
    elif job == "unemployed":
        age = np.random.randint(25, 66)
    else:
        age = int(np.clip(np.random.normal(loc=45, scale=12), 18, 90))
    # Inject anomaly: Occasionally assign an unrealistic age.
    if np.random.rand() < anomaly_rate_general:
        age = np.random.choice([np.random.randint(10, 20), np.random.randint(90, 100)])
    ages.append(age)

    # -- Marital Status --
    # Determine marital status based on age.
    if age < 30:
        marital = np.random.choice(["single", "married", "divorced"], p=[0.70, 0.20, 0.10])
    elif age <= 60:
        marital = np.random.choice(["married", "single", "divorced"], p=[0.70, 0.20, 0.10])
    else:
        marital = np.random.choice(["married", "divorced", "single"], p=[0.50, 0.40, 0.10])
    maritals.append(marital)

    # -- Education Level --
    # Choose education level based on job-specific probabilities.
    edu_probs = education_probs_by_job[job]
    education = np.random.choice(education_levels, p=edu_probs)
    educations.append(education)

    # -- Income Generation --
    # Generate monthly income within the range for the job.
    low, high = income_ranges[job]
    income = np.random.randint(low, high + 1)
    # Inject anomaly: Occasionally assign a negative or unusually high income.
    if np.random.rand() < anomaly_rate_general:
        income = np.random.choice([-np.random.randint(0, 1000), np.random.randint(high + 200, high + 5000)])
    incomes.append(income)

    # -- Credit Default Calculation --
    # Calculate default probability based on education and adjust by marital status and creation date.
    if education == "primary":
        default_prob = 0.06
    elif education == "secondary":
        default_prob = 0.04
    else:
        default_prob = 0.02
    if marital == "married":
        default_prob *= 0.8
    elif marital == "single":
        default_prob *= 1.1
    elif marital == "divorced":
        default_prob *= 1.2
    if created_at.year < 2018:
        default_prob *= 1.2
    default_prob *= np.random.uniform(0.9, 1.1)
    default_prob = min(default_prob, 0.5)
    credit_default = "yes" if np.random.rand() < default_prob else "no"
    credit_defaults.append(credit_default)

    # -- Customer Segmentation --
    # Assign a segment using the helper function.
    segment = assign_segment(age, job, education, region)
    segments.append(segment)

    # ----------------------------- #
    # 5. BANKING PRODUCT OWNERSHIP  #
    # ----------------------------- #

    # -- Credit Card --
    cc_prob = 0.6
    cc_prob += 0.1 if job not in ["student", "unemployed"] else -0.1
    cc_prob += 0.1 if 25 <= age <= 65 else -0.05
    cc_prob += 0.1 if income > 3000 else 0
    cc_prob += 0.1 if segment == "High-Value" else 0
    cc_prob = max(min(cc_prob, 0.95), 0)
    credit_card = "yes" if np.random.rand() < cc_prob else "no"
    # Inject noise: With 5% chance, force product ownership to "yes"
    if np.random.rand() < anomaly_rate_products:
        credit_card = "yes"
    credit_cards.append(credit_card)

    # -- Personal Loan --
    pl_prob = 0.4
    pl_prob += 0.2 if income < 3000 else 0
    pl_prob += 0.1 if segment == "Budget-Conscious" else 0
    pl_prob += 0.05 if education == "primary" else 0
    pl_prob += 0.1 if 25 <= age <= 55 else 0
    pl_prob = max(min(pl_prob, 0.9), 0)
    personal_loan = "yes" if np.random.rand() < pl_prob else "no"
    if np.random.rand() < anomaly_rate_products:
        personal_loan = "yes"
    personal_loans.append(personal_loan)

    # -- Mortgage --
    m_prob = 0.2
    m_prob += 0.2 if (age > 30 and marital == "married") else 0
    m_prob += 0.15 if income > 4000 else 0
    m_prob += 0.1 if segment in ["High-Value", "Middle-Market"] else 0
    m_prob += 0.1 if region in ["Urban", "Suburban"] else 0
    m_prob = max(min(m_prob, 0.9), 0)
    mortgage = "yes" if np.random.rand() < m_prob else "no"
    # Inject anomaly: e.g., a student could have a mortgage via co-signing.
    if np.random.rand() < anomaly_rate_products:
        mortgage = "yes"
    mortgages.append(mortgage)

    # -- Savings Account --
    sa_prob = 0.9
    sa_prob -= 0.1 if job in ["student", "unemployed"] else 0
    sa_prob = max(min(sa_prob, 0.99), 0)
    savings_account = "yes" if np.random.rand() < sa_prob else "no"
    if np.random.rand() < anomaly_rate_products:
        savings_account = "yes"
    savings_accounts.append(savings_account)

    # -- Investment Product --
    inv_prob = 0.2
    inv_prob += 0.2 if income > 5000 else 0
    inv_prob += 0.1 if education == "tertiary" else 0
    inv_prob += 0.2 if segment == "High-Value" else 0
    inv_prob += 0.1 if job in ["management", "self-employed", "entrepreneur"] else 0
    inv_prob = max(min(inv_prob, 0.9), 0)
    investment_product = "yes" if np.random.rand() < inv_prob else "no"
    if np.random.rand() < anomaly_rate_products:
        investment_product = "yes"
    investment_products.append(investment_product)

    # -- Auto Loan --
    al_prob = 0.3
    al_prob += 0.2 if 25 <= age <= 65 else 0
    al_prob += 0.1 if income > 2500 else 0
    al_prob += 0.1 if job not in ["student", "retired", "unemployed"] else 0
    al_prob += 0.05 if region in ["Urban", "Suburban"] else 0
    al_prob = max(min(al_prob, 0.9), 0)
    auto_loan = "yes" if np.random.rand() < al_prob else "no"
    if np.random.rand() < anomaly_rate_products:
        auto_loan = "yes"
    auto_loans.append(auto_loan)

    # -- Wealth Management --
    wm_prob = 0.1
    wm_prob += 0.2 if income > 7000 else 0
    wm_prob += 0.2 if segment == "High-Value" else 0
    wm_prob += 0.1 if education == "tertiary" else 0
    wm_prob += 0.1 if job in ["management", "entrepreneur"] else 0
    wm_prob += 0.05 if region == "Urban" else 0
    wm_prob = max(min(wm_prob, 0.9), 0)
    wealth_management = "yes" if np.random.rand() < wm_prob else "no"
    if np.random.rand() < anomaly_rate_products:
        wealth_management = "yes"
    wealth_managements.append(wealth_management)

# ----------------------------------- #
# 6. CREATE THE FINAL DATAFRAME       #
# ----------------------------------- #

# Assemble all fields into a DataFrame.
customers_df = pd.DataFrame({
    "customer_id": customer_ids,
    "age": ages,
    "job": job_choices,
    "marital": maritals,
    "education": educations,
    "credit_default": credit_defaults,
    "customer_segment": segments,
    "region": customer_regions,
    "income": incomes,
    "created_at": created_ats,
    "credit_card": credit_cards,
    "personal_loan": personal_loans,
    "mortgage": mortgages,
    "savings_account": savings_accounts,
    "investment_product": investment_products,
    "auto_loan": auto_loans,
    "wealth_management": wealth_managements
})

# Optionally, save the DataFrame to a CSV file.
customers_df.to_csv("customers.csv", index=False)

# Display the first few rows to verify the output.
print(customers_df.head())

import matplotlib.pyplot as plt
import seaborn as sns

df = customers_df

# Identify categorical and numerical columns
categorical_cols = df.select_dtypes(include=['object', 'category']).columns
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns

# Set up the figure size
plt.figure(figsize=(12, 6))

# Plot categorical columns as bar charts
for col in categorical_cols:
    plt.figure(figsize=(10, 5))
    sns.countplot(data=df, x=col, order=df[col].value_counts().index, palette="viridis")
    plt.title(f"Distribution of {col}")
    plt.xticks(rotation=45)
    plt.show()

# Plot numerical columns as histograms
for col in numerical_cols:
    plt.figure(figsize=(10, 5))
    sns.histplot(df[col], bins=30, kde=True, color="blue")
    plt.title(f"Distribution of {col}")
    plt.xlabel(col)
    plt.ylabel("Frequency")
    plt.show()

import numpy as np
import pandas as pd
import datetime

# ---------------------------
# ASSUMPTION SETTINGS & HELPERS
# ---------------------------
# 1. Job activity factors: more active roles have higher transaction counts.
activity_factor_by_job = {
    "student": 0.7,
    "retired": 0.5,
    "unemployed": 0.6,
    "blue-collar": 1.0,
    "admin": 1.1,
    "technician": 1.1,
    "entrepreneur": 1.3,
    "services": 0.9,
    "self-employed": 1.2,
    "management": 1.3
}

def get_activity_factor(job):
    return activity_factor_by_job.get(job, 1.0)

# 2. Age factor: younger customers are assumed to be more active.
def get_age_factor(age):
    if age < 30:
        return 1.2
    elif age <= 60:
        return 1.0
    else:
        return 0.7

# 3. Baseline annual transaction count.
base_annual_tx = 400

# 4. High-value transaction threshold (reference).
HIGH_VALUE_THRESHOLD = 1000

# 5. Current date for simulation purposes.
current_date = pd.Timestamp.today()

# ---------------------------
# SIMULATION FUNCTION
# ---------------------------
def simulate_transaction_metrics(income, job, account_created, age):
    """
    Simulate transaction behavior metrics for a customer over 12, 6, and 3 month windows,
    based solely on age, job, and income. Also adds global metrics and peak month data.

    Instead of simple scaling, the 6m and 3m windows are simulated separately with noise.
    """


    # Combined multiplier from job and age factors.
    overall_multiplier = get_activity_factor(job) * get_age_factor(age)

    # Annual lambda for transactions.
    lambda_annual = base_annual_tx * overall_multiplier

    # Global metrics:
    deg = 1  # Lower degrees of freedom = fatter tails
    days_since_last_tx = int(max(0, min(200, int(np.random.standard_t(deg) * 10 + 15) / overall_multiplier)))
    account_lifetime = (current_date - pd.to_datetime(account_created)).days

    # Simulate annual transactions.
    tx_count_12m = np.random.poisson(lam=lambda_annual)
    tx_count_12m = max(tx_count_12m, 1)

    # Simulate 6m and 3m windows independently.
    factor_6 = max(np.random.normal(loc=1, scale=0.15), 0.5)
    factor_3 = max(np.random.normal(loc=1, scale=0.15), 0.5)
    tx_count_6m = np.random.poisson(lam=(lambda_annual / 2) * factor_6)
    tx_count_6m = max(tx_count_6m, 1)
    tx_count_3m = np.random.poisson(lam=(lambda_annual / 4) * factor_3)
    tx_count_3m = max(tx_count_3m, 1)

    # Average transaction amount: ~5% of income with mild lognormal noise.
    avg_tx_3m = income * 0.05 * np.random.lognormal(mean=0, sigma=0.1)
    avg_tx_6m = income * 0.05 * np.random.lognormal(mean=0, sigma=0.3)
    avg_tx_12m = income * 0.05 * np.random.lognormal(mean=0, sigma=0.5)

    # Total spending per window.
    total_spending_12m = tx_count_12m * avg_tx_12m
    total_spending_6m  = tx_count_6m  * avg_tx_6m
    total_spending_3m  = tx_count_3m  * avg_tx_3m

    # Transaction frequency (per month).
    freq_12m = tx_count_12m / 12.0
    freq_6m  = tx_count_6m  / 6.0
    freq_3m  = tx_count_3m  / 3.0

    # High-value transactions: probability based on income.
    if income > 8000:
        p_high = np.random.normal(loc=0.15, scale=0.02)
    elif income > 4000:
        p_high = np.random.normal(loc=0.12, scale=0.02)
    elif income > 2000:
        p_high = np.random.normal(loc=0.09, scale=0.02)
    else:
        p_high = np.random.normal(loc=0.06, scale=0.02)
    p_high = np.clip(p_high, 0.05, 0.2)

    hv_count_12m = np.random.binomial(tx_count_12m, p_high)
    hv_rate_12m  = hv_count_12m / tx_count_12m
    hv_count_6m  = np.random.binomial(tx_count_6m, p_high)
    hv_rate_6m   = hv_count_6m / tx_count_6m
    hv_count_3m  = np.random.binomial(tx_count_3m, p_high)
    hv_rate_3m   = hv_count_3m / tx_count_3m

    # Online transaction ratio based on age and job.
    if age < 30:
        base_online = 0.70
    elif age <= 60:
        base_online = 0.65
    else:
        base_online = 0.50
    if job in ['management', 'entrepreneur', 'self-employed']:
        base_online += 0.05
    elif job in ['retired', 'unemployed']:
        base_online -= 0.05
    online_ratio_12m = np.clip(np.random.normal(loc=base_online, scale=0.05), 0, 1)
    online_ratio_6m  = np.clip(np.random.normal(loc=base_online, scale=0.05), 0, 1)
    online_ratio_3m  = np.clip(np.random.normal(loc=base_online, scale=0.05), 0, 1)


    # Overall monthly frequency based on annual transactions.
    monthly_freq = lambda_annual / 12.0
    # Current balance as a multiple of income.
    current_balance = income * np.random.uniform(1, 3)

    # Peak month metrics (simulate seasonality index):
    # Choose a random peak month (1 to 12)
    peak_month = np.random.choice(np.arange(1, 13), p=[0.09, 0.08, 0.08,  0.07,  0.08,  0.09,  0.08,  0.07,  0.08,  0.08,  0.09,  0.11])
    # Peak month spending: assume it accounts for between 15% and 25% of annual spending.
    peak_month_factor_spending = np.random.uniform(0.15, 0.25)
    peak_month_spending = total_spending_12m * peak_month_factor_spending
    # Peak month frequency: assume the peak month has between 1.1 and 1.5 times the average monthly frequency.
    peak_month_factor_freq = np.random.uniform(1.1, 1.5)
    peak_month_frequency = monthly_freq * peak_month_factor_freq
    peak_month_frequency = int(peak_month_frequency)

    if (days_since_last_tx >= 90) and (days_since_last_tx <= 180):
        tx_count_3m = 0
        total_spending_3m = 0
        avg_tx_3m = 0
        freq_3m = 0
        hv_rate_3m = 0
        online_ratio_3m = 0

    elif days_since_last_tx > 180:
        tx_count_6m = 0
        total_spending_6m = 0
        avg_tx_6m = 0
        freq_6m = 0
        hv_rate_6m = 0
        online_ratio_6m = 0


    # Compile all metrics into a dictionary.
    metrics = {
        # Global metrics (placed first)
        "days_since_last_transaction": days_since_last_tx,
        "account_lifetime_days": account_lifetime,
        "current_balance": current_balance,
        "peak_month": peak_month,
        "peak_month_spending": peak_month_spending,
        "peak_month_frequency": peak_month_frequency,

        # 3-month window metrics.
        "total_transactions_3m": tx_count_3m,
        "total_transaction_amount_3m": total_spending_3m,
        "average_transaction_amount_3m": avg_tx_3m,
        "transaction_frequency_3m": freq_3m,
        "high_value_transaction_rate_3m": hv_rate_3m,
        "online_transaction_ratio_3m": online_ratio_3m,

        # 6-month window metrics.
        "total_transactions_6m": tx_count_6m,
        "total_transaction_amount_6m": total_spending_6m,
        "average_transaction_amount_6m": avg_tx_6m,
        "transaction_frequency_6m": freq_6m,
        "high_value_transaction_rate_6m": hv_rate_6m,
        "online_transaction_ratio_6m": online_ratio_6m,

        # 12-month window metrics.
        "total_transactions_12m": tx_count_12m,
        "total_transaction_amount_12m": total_spending_12m,
        "average_transaction_amount_12m": avg_tx_12m,
        "transaction_frequency_12m": freq_12m,
        "high_value_transaction_rate_12m": hv_rate_12m,
        "online_transaction_ratio_12m": online_ratio_12m
    }

    return metrics

# ---------------------------
# GENERATE THE TRANSACTION SUMMARY TABLE
# ---------------------------
# Assuming customers_df exists with columns: customer_id, job, age, income, created_at
summary_list = []
for idx, cust in customers_df.iterrows():
    metrics = simulate_transaction_metrics(cust['income'], cust['job'], cust['created_at'], cust['age'])
    row = {"customer_id": cust['customer_id']}
    row.update(metrics)
    summary_list.append(row)

transactions_summary_df = pd.DataFrame(summary_list)

# ---------------------------
# REORDER COLUMNS: Global metrics first, then group metrics by name (3m, 6m, 12m).
# ---------------------------
global_cols = ["customer_id", "days_since_last_transaction", "current_balance",
               "peak_month", "peak_month_spending", "peak_month_frequency"]

# Define the base metric names.
metric_names = ["total_transactions", "total_transaction_amount", "average_transaction_amount", "transaction_frequency",
                "high_value_transaction_rate", "online_transaction_ratio"]
periods = ["3m", "6m", "12m"]

# Build list of grouped columns by metric name.
grouped_cols = []
for metric in metric_names:
    for period in periods:
        grouped_cols.append(f"{metric}_{period}")

# Final column order.
final_columns = global_cols + grouped_cols

transactions_summary_df = transactions_summary_df[final_columns]

# Optionally, save the churn table to a CSV file.
transactions_summary_df.to_csv("transactions_summary.csv", index=False)

# Display a sample of the resulting summary table.
print(transactions_summary_df.head())

df = transactions_summary_df

# Identify categorical and numerical columns
categorical_cols = df.select_dtypes(include=['object', 'category']).columns
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns

# Set up the figure size
plt.figure(figsize=(12, 6))

# Plot categorical columns as bar charts
for col in categorical_cols:
    plt.figure(figsize=(10, 5))
    sns.countplot(data=df, x=col, order=df[col].value_counts().index, palette="viridis")
    plt.title(f"Distribution of {col}")
    plt.xticks(rotation=45)
    plt.show()

# Plot numerical columns as histograms
for col in numerical_cols:
    plt.figure(figsize=(10, 5))
    sns.histplot(df[col], bins=30, kde=True, color="blue")
    plt.title(f"Distribution of {col}")
    plt.xlabel(col)
    plt.ylabel("Frequency")
    plt.show()

import numpy as np
import pandas as pd
import datetime
from faker import Faker

fake = Faker()

# ------------------------------------------------------------------------------
# PREPARATION:
# Assume the following DataFrames exist in memory:
#   - customers_df: 5,000 customers with at least columns:
#         customer_id, customer_segment, job, age, income, created_at, etc.
#   - transactions_summary_df: with columns including:
#         customer_id, days_since_last_transaction, total_transactions_6m, total_transaction_amount_6m
# ------------------------------------------------------------------------------
merged_df = customers_df.merge(
    transactions_summary_df[['customer_id', 'days_since_last_transaction', 'total_transactions_6m', 'total_transaction_amount_6m']],
    on='customer_id',
    how='left'
)
merged_df['total_transactions_6m'] = merged_df['total_transactions_6m'].fillna(0)
merged_df['total_transaction_amount_6m'] = merged_df['total_transaction_amount_6m'].fillna(0)
merged_df['days_since_last_transaction'] = merged_df['days_since_last_transaction'].fillna(0)

# Compute churn risk using five assumptions:
def compute_churn_risk(row):
    # 1. Lower transaction frequency increases churn risk.
    freq = row['total_transactions_6m'] / 6.0  # avg per month
    risk_freq = 1 - min(freq, 1)  # if avg < 1 tx/month, higher risk.

    # 2. Lower total spending increases churn risk.
    spending = row['total_transaction_amount_6m']
    risk_spending = 1 - min(spending / 500, 1)  # spending < $500 means higher risk.

    # 3. Newer accounts are riskier.
    account_age_years = (pd.to_datetime("2023-12-31") - pd.to_datetime(row['created_at'])).days / 365.0
    risk_tenure = 1 if account_age_years < 1 else (0.5 if account_age_years < 3 else 0)

    # 4. Customer segment: Budget-Conscious customers have higher churn risk.
    segment = row.get('customer_segment', 'Middle-Market')
    risk_segment = {"High-Value": 0.1, "Middle-Market": 0.3, "Budget-Conscious": 0.5,
                    "Young Professionals": 0.4, "Retired": 0.2}.get(segment, 0.3)

    # 5. Lower income increases risk.
    risk_income = 1 - min(row['income'] / 5000, 1)

    # Final churn score (weighted average)
    score = 0.2 * (0.3 * risk_freq + 0.2 * risk_spending +
                   0.2 * risk_tenure + 0.2 * risk_segment + 0.1 * risk_income)
    return min(max(score, 0), 1)

merged_df['churn_risk'] = merged_df.apply(compute_churn_risk, axis=1)

# New function to determine churn status.
def determine_churn_status(row):
    # If customer hasn't transacted for 90+ days, override with 90% chance to churn.
    if row['days_since_last_transaction'] >= 160:
        return "Yes" if np.random.rand() < 0.80 else "No"
    elif row['days_since_last_transaction'] < 15:
        return "No"
    else:
        return "Yes" if np.random.rand() < row['churn_risk'] else "No"

merged_df['churn_status'] = merged_df.apply(determine_churn_status, axis=1)

# New churn date generation: if churned, generate a churn date.
def generate_churn_date(row):
    if row['churn_status'] == "Yes":
        start_date = datetime.date.today() - datetime.timedelta(days=row['days_since_last_transaction'])
        end_date = datetime.date.today() - datetime.timedelta(days=15)
        return fake.date_between(start_date=start_date, end_date=end_date)
    else:
        return None


merged_df['churn_date'] = merged_df.apply(generate_churn_date, axis=1)

# Create churn_df with only the required columns.
merged_df = merged_df.reset_index(drop=True)
merged_df['churn_id'] = merged_df.index + 1
churn_df = merged_df[['churn_id', 'customer_id', 'churn_status', 'churn_date']]

# Optionally, save the churn table to a CSV file.
churn_df.to_csv("churn.csv", index=False)

print(churn_df.head())

df = churn_df

# Identify categorical and numerical columns
categorical_cols = df.select_dtypes(include=['object', 'category']).columns
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns

# Set up the figure size
plt.figure(figsize=(12, 6))

# Plot categorical columns as bar charts
for col in categorical_cols:
    plt.figure(figsize=(10, 5))
    sns.countplot(data=df, x=col, order=df[col].value_counts().index, palette="viridis")
    plt.title(f"Distribution of {col}")
    plt.xticks(rotation=45)
    plt.show()

# Plot numerical columns as histograms
for col in numerical_cols:
    plt.figure(figsize=(10, 5))
    sns.histplot(df[col], bins=30, kde=True, color="blue")
    plt.title(f"Distribution of {col}")
    plt.xlabel(col)
    plt.ylabel("Frequency")
    plt.show()

import numpy as np
import pandas as pd
import datetime
from faker import Faker

fake = Faker()

usage_data = []  # List to collect usage metrics per customer

for idx, cust in customers_df.iterrows():
    customer_id = cust['customer_id']
    age = cust.get('age', 40)
    income = cust.get('income', 3000)
    job = cust.get('job', 'unknown')

    # Assumption 1: Mobile App Adoption by Age.
    if age < 40:
        has_mobile_app = "Yes" if np.random.rand() < 0.95 else "No"
    elif age <= 60:
        has_mobile_app = "Yes" if np.random.rand() < 0.80 else "No"
    else:
        has_mobile_app = "Yes" if np.random.rand() < 0.60 else "No"

    # Assumption 2: Almost everyone has web banking.
    has_web_account = "Yes" if np.random.rand() < 0.85 else "No"

    # Assumption 3 & 8: Mobile logins per week (as integer).
    if has_mobile_app == "Yes":
        base_mobile_logins = 10 if age < 40 else 6
        if job.lower() in ["entrepreneur", "management", "young professional"]:
            base_mobile_logins *= 1.1
        mobile_logins_per_week = int(round(np.random.normal(loc=base_mobile_logins, scale=1)))
    else:
        mobile_logins_per_week = 0

    # Assumption 4: Web logins per week.
    if has_web_account == "Yes":
        base_web_logins = 8 if age > 50 else 5
        web_logins_per_week = int(round(np.random.normal(loc=base_web_logins, scale=0.8)))
    else:
        web_logins_per_week = 0

    # Assumption 5: Average mobile session duration (in minutes).
    if has_mobile_app == "Yes":
        base_mobile_duration = 3 if age < 40 else 4
        base_mobile_duration *= 1.05 if income > 5000 else 1.0
        avg_mobile_session_duration = max(round(np.random.normal(loc=base_mobile_duration, scale=1.5), 1), 0)
    else:
        avg_mobile_session_duration = 0

    # Assumption 6: Average web session duration (in minutes).
    if has_web_account == "Yes":
        base_web_duration = 6 if age > 50 else 5
        avg_web_session_duration = max(round(np.random.normal(loc=base_web_duration, scale=2), 1), 0)
    else:
        avg_web_session_duration = 0

    # Assumption 7 & 9: Last login timestamps (with recency tied to activity).
    now = datetime.datetime.now()
    if has_mobile_app == "Yes" and mobile_logins_per_week > 0:
        last_mobile_login = fake.date_time_between(start_date='-10d', end_date='now')
    else:
        last_mobile_login = None
    if has_web_account == "Yes" and web_logins_per_week > 0:
        last_web_login = fake.date_time_between(start_date='-30d', end_date='now')
    else:
        last_web_login = None

    usage_data.append({
        "customer_id": customer_id,
        "has_mobile_app": has_mobile_app,
        "has_web_account": has_web_account,
        "mobile_logins_per_week": mobile_logins_per_week,
        "web_logins_per_week": web_logins_per_week,
        "avg_mobile_session_duration": avg_mobile_session_duration,  # in minutes
        "avg_web_session_duration": avg_web_session_duration,        # in minutes
        "last_mobile_login": last_mobile_login,
        "last_web_login": last_web_login
    })

digital_usage_df = pd.DataFrame(usage_data)

# ----- Incorporate churn info -----
# Assume churn_df exists with at least columns: customer_id, churn_status, churn_date.
# Convert churn_date to datetime if not already.
churn_df['churn_date'] = pd.to_datetime(churn_df['churn_date'], errors='coerce')

# Merge digital usage with churn table.
merged_usage = digital_usage_df.merge(churn_df[['customer_id', 'churn_status', 'churn_date']],
                                      on="customer_id", how="left")

# Function to adjust a login date to be before churn_date.
def adjust_last_login(row, login_col):
    # Only adjust if customer has churned and both dates exist.
    if row.get('churn_status') == "Yes" and pd.notnull(row.get('churn_date')) and pd.notnull(row.get(login_col)):
        churn_date = pd.to_datetime(row['churn_date']).date()
        login_date = pd.to_datetime(row[login_col]).date()
        # If login_date is on or after churn_date, generate a new login date before churn.
        if login_date >= churn_date:
            # Define a window: between (churn_date - 30 days) and (churn_date - 1 day).
            start_adj = churn_date - datetime.timedelta(days=30)
            end_adj = churn_date - datetime.timedelta(days=1)
            # Ensure start_adj is before end_adj.
            if start_adj >= end_adj:
                new_date = end_adj
            else:
                new_date = fake.date_time_between(start_date=start_adj, end_date=end_adj)
            return new_date
    return row[login_col]

merged_usage['last_mobile_login'] = merged_usage.apply(lambda row: adjust_last_login(row, 'last_mobile_login'), axis=1)
merged_usage['last_web_login'] = merged_usage.apply(lambda row: adjust_last_login(row, 'last_web_login'), axis=1)

# Optionally, save the updated digital usage table.
merged_usage.drop(columns=['churn_status', 'churn_date'], inplace=True)

# Optionally, save the churn table to a CSV file.
merged_usage.to_csv("digital_usage.csv", index=False)

print(merged_usage.head())

df = digital_usage_df

# Identify categorical and numerical columns
categorical_cols = df.select_dtypes(include=['object', 'category']).columns
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns

# Set up the figure size
plt.figure(figsize=(12, 6))

# Plot categorical columns as bar charts
for col in categorical_cols:
    plt.figure(figsize=(10, 5))
    sns.countplot(data=df, x=col, order=df[col].value_counts().index, palette="viridis")
    plt.title(f"Distribution of {col}")
    plt.xticks(rotation=45)
    plt.show()

# Plot numerical columns as histograms
for col in numerical_cols:
    plt.figure(figsize=(10, 5))
    sns.histplot(df[col], bins=30, kde=True, color="blue")
    plt.title(f"Distribution of {col}")
    plt.xlabel(col)
    plt.ylabel("Frequency")
    plt.show()

import numpy as np
import pandas as pd
import datetime
from faker import Faker

fake = Faker()  # Using default en_US locale (US-based)

# ------------------------------------------------------------------------------
# PREPARATION:
# Assume customers_df exists (with 5000 rows) and contains at least:
#   customer_id, customer_segment, job, age, income, created_at, etc.
# For example, you might have loaded it with:
# customers_df = pd.read_csv("synthetic_customers_enhanced.csv")
# ------------------------------------------------------------------------------
# Also, we define our known customer segments.
segments = ["High-Value", "Budget-Conscious", "Middle-Market", "Young Professionals", "Retired"]

# ------------------------------------------------------------------------------
# BANKING PRODUCTS:
# Each product has an ID and a name.
banking_products = [
    {"product_id": 101, "product": "Credit Card"},
    {"product_id": 102, "product": "Personal Loan"},
    {"product_id": 103, "product": "Mortgage"},
    {"product_id": 104, "product": "Savings Account"},
    {"product_id": 105, "product": "Investment Product"},
    {"product_id": 106, "product": "Auto Loan"},
    {"product_id": 107, "product": "Wealth Management"}
]

# ------------------------------------------------------------------------------
# ASSUMPTION: PRODUCT & CAMPAIGN TYPE MAPPING BY SEGMENT
# ------------------------------------------------------------------------------
target_segment_product_mapping = {
    "High-Value": [105, 107],         # Investment and Wealth Management
    "Budget-Conscious": [101, 102, 106],     # Credit Card, Personal Loan, and Auto Loan
    "Middle-Market": [101, 104, 102, 106],    # Credit Card, Savings, Personal Loan, and Auto Loan
    "Young Professionals": [101, 102, 106],   # Credit Card, Personal Loan, and Auto Loan
    "Retired": [103, 104, 107]           # Mortgage, Savings, Wealth Management
}

# Updated campaign type mapping: Social Media removed; Mobile App Notifications added.
target_segment_campaign_type_mapping = {
    "High-Value": ["Email", "Mobile App Notifications"],
    "Budget-Conscious": ["SMS", "Email", "Mobile App Notifications"],
    "Middle-Market": ["Email", "SMS", "Mobile App Notifications"],
    "Young Professionals": ["SMS", "Mobile App Notifications"],
    "Retired": ["Direct Mail", "Email"]  # Retired customers do not get Mobile App Notifications.
}

# ------------------------------------------------------------------------------
# NUMBER OF CAMPAIGNS TO GENERATE
# ------------------------------------------------------------------------------
n_campaigns = 50

# ------------------------------------------------------------------------------
# PREPARE LISTS FOR CAMPAIGN FIELDS
# ------------------------------------------------------------------------------
campaign_ids = np.arange(1, n_campaigns + 1)
campaign_customer_segments = []
campaign_names = []
campaign_types_list = []
start_dates = []
end_dates = []
target_audiences = []
offer_details_list = []
recommended_product_ids = []
campaign_budgets = []
total_customers_reached = []
total_conversions = []
conversion_rates = []
total_campaign_costs = []
total_revenue_generated = []
ROIs = []
customer_lifetime_values = []
cost_per_conversions = []
created_ats = []

# ------------------------------------------------------------------------------
# HELPER FUNCTIONS
# ------------------------------------------------------------------------------
def generate_campaign_dates():
    """
    Each campaign is created in late 2023, starts between early Jan and mid-Feb 2024,
    and lasts between 30 and 60 days.
    """
    creation_date = fake.date_between(start_date=datetime.date(2024, 12, 1),
                                        end_date=datetime.date(2024, 12, 31))
    start_date = fake.date_between(start_date=datetime.date(2022, 1, 1),
                                   end_date=datetime.date(2024, 6, 1))
    duration = np.random.randint(30, 61)
    end_date = start_date + datetime.timedelta(days=duration)
    return creation_date, start_date, end_date

def simulate_engagement_metrics():
    """
    Simulate baseline engagement metrics:
      - Customers reached: 1,000 to 10,000.
      - Conversion rate: between 2% and 8%.
      - Campaign cost: between $10,000 and $100,000.
      - Revenue multiplier: between 1.2 and 3.0.
      - Customer lifetime value (CLV): between $500 and $5000.
      - Cost per conversion computed if conversions > 0.
    """
    reached = np.random.randint(1000, 10001)
    conv_rate = np.random.uniform(0.02, 0.08)
    conversions = int(reached * conv_rate)
    cost = np.random.uniform(10000, 100000)
    revenue = cost * np.random.uniform(1.2, 3.0)
    ROI = (revenue - cost) / cost if cost > 0 else 0
    clv = np.random.uniform(500, 5000)
    cost_per_conv = cost / conversions if conversions > 0 else cost
    return reached, conversions, conv_rate, cost, revenue, ROI, clv, cost_per_conv

# ------------------------------------------------------------------------------
# GENERATE CAMPAIGN RECORDS
# ------------------------------------------------------------------------------
for i in range(n_campaigns):
    # --- Assumption 1: Targeted Customer Segment
    target_segment = np.random.choice(segments)
    campaign_customer_segments.append(target_segment)

    # --- Assumption 2: Campaign Type Based on Customer Segment
    available_types = target_segment_campaign_type_mapping[target_segment]
    campaign_type = np.random.choice(available_types)
    campaign_types_list.append(campaign_type)

    # --- Assumption 3: Product Relevance for the Target Segment
    product_choices = target_segment_product_mapping[target_segment]
    recommended_product_id = np.random.choice(product_choices)
    recommended_product_ids.append(recommended_product_id)

    # Get product name for offer details.
    product_name = next((p["product"] for p in banking_products if p["product_id"] == recommended_product_id), "Product")

    # --- Assumption 4: Offer Details & Naming
    offer_details = f"Special offer on our {product_name} tailored for {target_segment} customers."
    offer_details_list.append(offer_details)
    campaign_name = f"{product_name} Campaign for {target_segment}"
    campaign_names.append(campaign_name)

    # --- Assumption 5: Campaign Timing and Duration
    c_created, c_start, c_end = generate_campaign_dates()
    created_ats.append(c_created)
    start_dates.append(c_start)
    end_dates.append(c_end)

    # --- Assumption 6: Target Audience Description
    target_audience = f"{target_segment} customers interested in {product_name} solutions."
    target_audiences.append(target_audience)

    # --- Assumption 7: Budget Allocation Based on Segment
    if target_segment == "High-Value":
        budget = np.random.uniform(50000, 100000)
    elif target_segment == "Retired":
        budget = np.random.uniform(30000, 70000)
    else:
        budget = np.random.uniform(20000, 80000)
    campaign_budgets.append(round(budget, 2))

    # --- Assumption 8: Simulated Engagement Metrics (Baseline)
    (reached, conversions, conv_rate, cost, revenue, ROI, clv, cost_per_conv) = simulate_engagement_metrics()

    # --- Assumption 9: Adjustments by Marketing Method with Noise
    # Introduce different multipliers based on campaign type, with additional noise.
    noise_factor = np.random.normal(loc=1, scale=0.05)  # small random noise
    if campaign_type == "Mobile App Notifications":
        conv_rate *= 1.1 * noise_factor    # 10% higher conversion rate
        cost *= 0.9 * noise_factor         # 10% lower cost
        revenue = cost * np.random.uniform(1.3, 3.2) * noise_factor  # higher revenue multiplier
        ROI = (revenue - cost) / cost if cost > 0 else 0
    elif campaign_type == "SMS":
        conv_rate *= 0.95 * noise_factor   # 5% lower conversion rate
        cost *= 0.85 * noise_factor        # 15% lower cost
        revenue = cost * np.random.uniform(1.1, 2.5) * noise_factor
        ROI = (revenue - cost) / cost if cost > 0 else 0
    elif campaign_type == "Direct Mail":
        conv_rate *= 0.9 * noise_factor    # 10% lower conversion rate
        cost *= 1.2 * noise_factor         # 20% higher cost
        revenue = cost * np.random.uniform(1.0, 2.0) * noise_factor
        ROI = (revenue - cost) / cost if cost > 0 else 0
    elif campaign_type == "Email":
        conv_rate *= 1.0 * noise_factor    # baseline conversion rate
        cost *= 1.0 * noise_factor         # baseline cost
        revenue = cost * np.random.uniform(1.2, 3.0) * noise_factor
        ROI = (revenue - cost) / cost if cost > 0 else 0

    total_customers_reached.append(reached)
    total_conversions.append(conversions)
    conversion_rates.append(round(conv_rate, 4))
    total_campaign_costs.append(round(cost, 2))
    total_revenue_generated.append(round(revenue, 2))
    ROIs.append(round(ROI, 4))
    customer_lifetime_values.append(round(clv, 2))
    cost_per_conversions.append(round(cost_per_conv, 2))

# ------------------------------------------------------------------------------
# ASSEMBLE THE CAMPAIGNS DATAFRAME
# ------------------------------------------------------------------------------
campaigns_df = pd.DataFrame({
    "campaign_id": campaign_ids,
    "customer_segment": campaign_customer_segments,
    "campaign_name": campaign_names,
    "campaign_type": campaign_types_list,
    "start_date": start_dates,
    "end_date": end_dates,
    "target_audience": target_audiences,
    "offer_details": offer_details_list,
    "recommended_product_id": recommended_product_ids,
    "recommended_product_name": [next((p["product"] for p in banking_products if p["product_id"] == pid), "Product") for pid in recommended_product_ids],
    "campaign_budget": campaign_budgets,
    "total_campaign_cost": total_campaign_costs,
    "total_revenue_generated": total_revenue_generated,
    "ROI": ROIs,
    "customer_lifetime_value": customer_lifetime_values,
    "cost_per_conversion": cost_per_conversions,
    "created_at": created_ats
})

# ------------------------------------------------------------------------------
# (OPTIONAL) SAVE THE CAMPAIGNS DATAFRAME
# ------------------------------------------------------------------------------
campaigns_df.to_csv("campaigns.csv", index=False)

# ------------------------------------------------------------------------------
# VERIFICATION: Display the first few rows.
# ------------------------------------------------------------------------------
print(campaigns_df.head())

df = campaigns_df

# Identify categorical and numerical columns
categorical_cols = df.select_dtypes(include=['object', 'category']).columns
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns

# Set up the figure size
plt.figure(figsize=(12, 6))

# Plot categorical columns as bar charts
for col in categorical_cols:
    plt.figure(figsize=(10, 5))
    sns.countplot(data=df, x=col, order=df[col].value_counts().index, palette="viridis")
    plt.title(f"Distribution of {col}")
    plt.xticks(rotation=45)
    plt.show()

# Plot numerical columns as histograms
for col in numerical_cols:
    plt.figure(figsize=(10, 5))
    sns.histplot(df[col], kde=True, color="blue")
    plt.title(f"Distribution of {col}")
    plt.xlabel(col)
    plt.ylabel("Frequency")
    plt.show()

import numpy as np
import pandas as pd
import datetime
import random
from faker import Faker

fake = Faker()  # Using default en_US locale

# ------------------------------------------------------------------------------
# PREPARATION:
# Assume the following DataFrames exist in memory:
#   - customers_df: 5,000 customers with at least columns:
#         customer_id, customer_segment, job, age, income, created_at, etc.
#   - campaigns_df: Campaigns with at least columns:
#         campaign_id, customer_segment, start_date, end_date, campaign_type, etc.
#   - digital_usage_df: Digital usage metrics per customer including:
#         customer_id, has_mobile_app, etc.
# ------------------------------------------------------------------------------
# (They are assumed to be already loaded in memory.)

# ------------------------------------------------------------------------------
# SIMULATION PARAMETERS FOR ENGAGEMENT EVENTS:
# ------------------------------------------------------------------------------
engagement_events = []  # List to store engagement event dictionaries
engagement_id_counter = 1

# Engagement indicator mappings (we'll still use these to sample an "indicator" in Assumption 2,
# even though later we produce binary outcomes for each stage).
engagement_types_by_campaign = {
    "Email": {"email opened": 0.35, "email clicked": 0.35, "link clicked": 0.15, "no engagement": 0.15},
    "SMS": {"sms delivered": 0.25, "sms clicked": 0.45, "sms replied": 0.15, "no engagement": 0.15},
    "Mobile App Notifications": {"notification received": 0.25, "notification opened": 0.45, "notification dismissed": 0.15, "no engagement": 0.15},
    "Direct Mail": {"mail received": 0.35, "mail read": 0.35, "call initiated": 0.15, "no engagement": 0.15}
}

# Base conversion probabilities per indicator (used later for conversion_status).
base_conversion_probs = {
    "email opened": 0.10,
    "email clicked": 0.40,
    "link clicked": 0.70,
    "sms delivered": 0.05,
    "sms clicked": 0.20,
    "sms replied": 0.50,
    "notification received": 0.05,
    "notification opened": 0.40,
    "notification dismissed": 0.10,
    "mail received": 0.05,
    "mail read": 0.20,
    "call initiated": 0.50,
    "no engagement": 0.0
}

def get_indicator_mapping(camp_type):
    mapping = engagement_types_by_campaign.get(camp_type,
                {"email opened": 0.5, "email clicked": 0.3, "link clicked": 0.2, "no engagement": 0.0})
    return list(mapping.keys()), list(mapping.values())

# Predefine peak months and months array.
peak_months = [12, 1, 2, 11, 7]
months_arr = np.arange(1, 13)

# ------------------------------------------------------------------------------
# SIMULATE ENGAGEMENT EVENTS PER CAMPAIGN
# ------------------------------------------------------------------------------
for idx, campaign in campaigns_df.iterrows():
    campaign_id = campaign["campaign_id"]
    target_segment = campaign["customer_segment"]
    camp_start = pd.to_datetime(campaign["start_date"])
    camp_end = pd.to_datetime(campaign["end_date"])
    camp_type = campaign["campaign_type"]

    camp_start_date = camp_start.date()
    camp_end_date = camp_end.date()

    # Generate between 100 and 500 engagement events per campaign.
    num_events = np.random.randint(100, 1001)

    for _ in range(num_events):
        # --- Assumption 1: Randomly pick a customer.
        customer = customers_df.sample(n=1).iloc[0]
        customer_id = customer["customer_id"]
        # For Mobile App Notifications, ensure the customer actually has a mobile app.
        if camp_type == "Mobile App Notifications":
            while digital_usage_df[digital_usage_df["customer_id"] == customer_id]["has_mobile_app"].values[0] != "Yes":
                customer = customers_df.sample(n=1).iloc[0]
                customer_id = customer["customer_id"]
        match_status = "match" if customer["customer_segment"] == target_segment else "nonmatch"

        # --- Assumption 2: Determine engagement indicator.
        keys, probs = get_indicator_mapping(camp_type)
        indicator = np.random.choice(keys, p=probs)

        # --- Assumption 3: Engagement Score.
        if indicator == "no engagement":
            engagement_score = 0.0
        else:
            # For now, use a random score; in practice, this could be a composite metric.
            engagement_score = round(np.random.uniform(0.1, 1.0), 2)

        # --- Assumption 4: Conversion Probability Calculation.
        base_conv_prob = base_conversion_probs.get(indicator, 0.1)
        match_multiplier = np.random.normal(1.1, 0.05) if match_status=="match" else np.random.normal(0.9, 0.05)
        income_multiplier = 1.1 if customer["income"] > 5000 else (0.9 if customer["income"] < 2000 else 1.0)
        job_multiplier = np.random.normal(0.9, 0.03) if customer["job"].lower() in ["student", "unemployed"] else (np.random.normal(1.1, 0.03) if customer["job"].lower()=="entrepreneur" else 1.0)
        weights = np.array([np.random.uniform(0.02, 0.10) if m in peak_months else 0.01 for m in months_arr])
        weights /= weights.sum()
        chosen_month = np.random.choice(months_arr, p=weights)
        time_multiplier = np.random.normal(1.1, 0.03) if chosen_month in peak_months else np.random.normal(0.95, 0.03)

        conv_multiplier = match_multiplier * income_multiplier * job_multiplier * time_multiplier
        # Adjust conversion probability by multiplying by 0.4 (this factor can be tuned).
        conv_prob = np.clip(base_conv_prob * conv_multiplier * 0.4, 0, 1)
        if indicator == "no engagement":
            conv_prob = 0.0
        conversion_status = "Yes" if np.random.rand() < conv_prob else "No"

        # --- Assumption 5: Engagement Date/Time with Seasonal Trend.
        engagement_date_date = fake.date_between(start_date=camp_start_date, end_date=camp_end_date)
        if engagement_date_date.month not in peak_months and np.random.rand() < 0.3:
            possible_dates = [d.date() for d in pd.date_range(camp_start, camp_end) if d.month in peak_months]
            if possible_dates:
                engagement_date_date = random.choice(possible_dates)
        hour = int(np.clip(np.random.normal(13, 3), 0, 23))
        minute = np.random.randint(0, 60)
        second = np.random.randint(0, 60)
        engagement_date = datetime.datetime.combine(engagement_date_date, datetime.time(hour, minute, second))
        created_at = engagement_date
        engagement_time = engagement_date.strftime("%H:%M:%S")

        # --- New: Aggregated Channel Engagement Indicators as Binary Outcomes ---
        # For each channel, we simulate binary flags ("Yes"/"No") for:
        #   sent, delivered, opened, and clicked.
        # We assume:
        # - For Email: sent always "Yes"; delivered 95% chance; opened: 40% if match, 20% if nonmatch; clicked: 10% if match, 5% if nonmatch.
        channel = camp_type
        if camp_type == "Email":
            sent_status = "Yes"
            delivered_status = "Yes" if np.random.rand() < 0.98 else "No"
            opened_status = "Yes" if np.random.rand() < (0.40 if match_status=="match" else 0.20) else "No"
            clicked_status = "Yes" if np.random.rand() < (0.12 if match_status=="match" else 0.05) else "No"
        elif camp_type == "SMS":
            sent_status = "Yes"
            delivered_status = "Yes" if np.random.rand() < 0.99 else "No"
            opened_status = "Yes" if np.random.rand() < (0.35 if match_status=="match" else 0.15) else "No"
            clicked_status = "Yes" if np.random.rand() < (0.15 if match_status=="match" else 0.06) else "No"
        elif camp_type == "Mobile App Notifications":
            sent_status = "Yes"
            delivered_status = "Yes" if np.random.rand() < 0.92 else "No"
            opened_status = "Yes" if np.random.rand() < (0.50 if match_status=="match" else 0.25) else "No"
            clicked_status = "Yes" if np.random.rand() < (0.20 if match_status=="match" else 0.07) else "No"
        elif camp_type == "Direct Mail":
            sent_status = "Yes"
            delivered_status = "No"
            opened_status = "No"
            clicked_status = "No"
        else:
            # Default values for unknown channels.
            sent_status = "Yes"
            delivered_status = "Yes" if np.random.rand() < 0.90 else "No"
            opened_status = "Yes" if np.random.rand() < 0.40 else "No"
            clicked_status = "Yes" if np.random.rand() < 0.10 else "No"

        record = {
            "engagement_id": engagement_id_counter,
            "customer_id": customer_id,
            "campaign_id": campaign_id,
            "channel": channel,
            "sent": sent_status,
            "delivered": delivered_status,
            "opened": opened_status,
            "clicked": clicked_status,
            "engagement_date": engagement_date,
            "engagement_time": engagement_time,
            "engagement_score": engagement_score,
            "conversion_status": conversion_status,
            "created_at": created_at,
            "match_status": match_status
        }
        engagement_events.append(record)
        engagement_id_counter += 1

# ------------------------------------------------------------------------------
# ASSEMBLE THE CUSTOMER ENGAGEMENT DATAFRAME
# ------------------------------------------------------------------------------
customer_engagement_df = pd.DataFrame(engagement_events)

# Save to CSV.
customer_engagement_df.to_csv("customer_engagement.csv", index=False)

print(customer_engagement_df.head())

customer_engagement_df.info()

# df = customer_engagement_df

# # Identify categorical and numerical columns
# categorical_cols = df.select_dtypes(include=['object', 'category']).columns
# numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns

# # Set up the figure size
# plt.figure(figsize=(12, 6))

# # Plot categorical columns as bar charts
# for col in categorical_cols:
#     plt.figure(figsize=(10, 5))
#     sns.countplot(data=df, x=col, order=df[col].value_counts().index, palette="viridis")
#     plt.title(f"Distribution of {col}")
#     plt.xticks(rotation=45)
#     plt.show()

# # Plot numerical columns as histograms
# for col in numerical_cols:
#     plt.figure(figsize=(10, 5))
#     sns.histplot(df[col], kde=True, color="blue")
#     plt.title(f"Distribution of {col}")
#     plt.xlabel(col)
#     plt.ylabel("Frequency")
#     plt.show()

"""# Data"""

customers = pd.read_csv("customers.csv")
customers.drop(columns=['created_at'], inplace=True)
customers.to_csv("customers.csv", index=False)

customer_engagement = pd.read_csv("customer_engagement.csv")
customer_engagement.drop(columns=['engagement_score','created_at','match_status'], inplace=True)
customer_engagement.to_csv("customer_engagement.csv", index=False)

campaigns = pd.read_csv("campaigns.csv")
campaigns.drop(columns=['target_audience','offer_details','created_at','ROI',	'customer_lifetime_value','cost_per_conversion'], inplace=True)
campaigns.to_csv("campaigns.csv", index=False)